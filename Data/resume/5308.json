{"skills": ["apache sqoop", "streaming", "dom", "analysis", "spark sql", "reactjs", "api based", "mysql", "service management", "cloud infrastructure", "amazon aurora", "product management", "auto scaling", "elasticsearch", "stack", "oop", "yarn", "dataproc", "apache kafka", "implementation", "ansible", "workflow", "rest", "jms", "docker swarm", "scala", "project estimation", "testing", "kafka", "apache spark", "python", "project planning", "prince2", "findbugs", "monitoring security", "storage service", "data analysis", "identity and access", "security", "mllib", "continuous delivery", "project management", "dataflow", "apache ambari", "sysops", "strut", "devops", "cloudformation", "apache", "database", "ibm", "jdbc", "object oriented", "bigquery", "cloud", "cloudwatch", "docker", "hibernate", "tdd", "test driven development", "identity", "soapui", "management", "google cloud", "private cloud", "jvm", "machine learning", "it infrastructure", "ssl", "networking", "java", "git", "css", "administration", "parsing", "hue", "maven", "itil", "visualization", "jira", "html", "service layer", "kibana", "full stack development", "map reduce", "hbase", "compute", "data analytics", "dns", "banking", "spring boot", "jsp", "mockito", "effort estimation", "streaming data", "software development", "junit", "it service management", "hadoop", "user management", "mongodb", "mapreduce", "postgresql", "restful", "continuous integration", "json", "hdfs", "automation", "data storage", "security system", "tomcat", "ant", "j2ee", "kerberos", "oracle", "insurance", "kubernetes", "cloud data", "nosql", "angular", "docker compose", "jboss", "sql", "cassandra", "google cloud pubsub", "srs", "javascript", "sps", "analytics", "cisco", "enterprise architecture", "xml", "cloud service", "aws", "dynamodb", "checkstyle", "oracle soa suite", "enterprise", "jenkins", "data visualization", "information security", "jsf", "spring", "prometheus", "graph visualization", "load balancer"], "fulltext": "Deepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | \nSecurity (5X) \nPlease note that I have a 5 year Long Term Stay Visa in Vietnam already and don't need any work \npermit. \nExperience Summary: \n\uf06c 16 years of experience in the Software Development/IT/Infrastructure/Consulting using various enterprise \ntechnologies. \n\uf06c Senior Solutions Architect/Consultant having experience building and managing Production environments \nfor complex, high traffic B2C and B2B products across mobile and web for major Big clients like Equinix, ANZ, \nUOB, MOHH, MOE, Everything Everywhere, TARGET, Washington Mutual, SuperValu etc. \n\uf06c Hardcore working experience of being in DevOps, Cloud, Big Data, Full Stack, Digital Transformation \nprojects and Large Scale Implementations.  \n\uf06c Experienced with all aspects of Development and Administration of Big Data Production Deployment. \n\uf06c Extensive experience with Development and Administration of Multi Node HDFS and YARN Production Grade \nCluster. \n\uf06c Development of programs or scripts using Map-Reduce, PIG, Hive, Sqoop or Flume agents \n\uf06c Administration & Development Experience with all modules of Apache Spark (Scala, Python and Java) - RDD, \nDataFrame, Dataset, Spark SQL, Core Spark, MLlib, Spark Streaming. Setup, Install, Configure Multi Node \nSpark Cluster \n\uf06c Integration of Spark with Apache Kafka, Elasticsearch and Kibana for visualizations \n\uf06c NoSQL Databases - MongoDB, Cassandra, HBase, DynamoDB on AWS Cloud \n\uf06c Strong Experience working with all the types of SQL based Databases like PostgreSQL, Oracle and MySQL. \n\uf06c Kafka - Development and Administration of Multi Node production grade cluster, Writing Java Producers and \nConsumers, Kafka Connect Usage, Confluent Schema Registry, Kafka Streams Application Development, \nKafka Security (SSL, SASL Kerberos, ACL in ZooKeeper, Kafka Monitoring and Operations. \n\uf06c Google Cloud Services - Good exposure with Google Cloud Services like DataProc, BigTable, BigQuery, \nDataFlow, CloudSQL, Google Cloud PubSub on GCP \n\uf06c Extensive Development and Administration experience with all Amazon Web Services like Elastic Compute \nCloud (EC2), EBS, Simple Storage Service (S3),Route53,Elastic Load Balancer, Auto Scaling Groups, Placement \nGroups, Launch Configurations, CloudFront, Security Groups, EBS, CloudWatch, CloudTrail, EFS, Lambda, \nVirtual Private Cloud (VPC), Kinesis, DynamoDB, RDS, RedShift, SQS, SNS, SWF, Elasticsearch, \nCloudFormation, Elastic Beanstalk, OpsWorks, Continuous Integration & Deployment, Identity and Access \nManagement, Web Identity Federation, Amazon Aurora, Elastic Block Storage, STS, Data Pipeline. \n\uf06c Have very good hands-on experience working in DevOps, Building Continuous Integration and Delivery \npieplines using tools like Kubernetes, Docker, Jenkins, Maven, GIT, Ansible, Prometheus and JIRA \n\uf06c Kubernetes  - Solid Development and Administration experience with all aspects of Kubernetes (Multi Node \nProduction Grade Cluster, Pods, Services, Deployments, RepliceSets, Jobs, CronJobs, Persistent Volumes, \nIngress Controller, Namespaces, Resource Allocation using CPU, Memory, Security, Monitoring and \nOperations) \n\uf06c Docker - Experienced with all aspects and features of Multi Node Production Docker Cluster (Installation, \nConfiguration, Administration, Docker Swarm, Image Creation, Management, Registry, Writing Dockerfile, \nDocker Compose, Storage Volumes, Networking, Security) \n\uf06c Jenkins - Experienced with all aspects of Production Jenkins deployment (Installation, Configuration, \nAdministration, Jobs, Builds, Integrating with (GIT,Docker,Kubernetes), Notifications, Distributed Builds, \nContinuous Delivery Pipelines, Triggers, Pipeline Multibranch & Repository Scanning, Artifacts & \nFingerprinting) \n\uf06c Good exposure to DevOps on AWS using tools like CloudFormation, OpsWorks and Elastic BeanStalk. \n\uf06c Extensive experience with Full stack development using frontend/backend technologies like ReactJS, \nAngular, NodeJS, HTML/CSS, Java 5/6/7/8, RESTful APIs, various JavaScript libraries, frameworks. \n\uf06c Strong experience with Java/Java EE/Web Component/Web Service Development (JAX-WS, REST, JSON, \nApache CXF etc) \nDeepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | \nSecurity (5X) \n\uf06c Strong Working knowledge Object Oriented Programming(OOP) and Object Oriented Analysis and Design \n(OOAD). \n\uf06c Practical experience in most of the following areas - JSF, Spring, Spring Boot, Hibernate, MyBatis and other \nJava-based frameworks, Threads, JDBC/SQL, Server side development, JSP/Servlet, JNDI, JSF, Struts etc. \n\uf06c Very good exposure using Test Driven Development (TDD) using frameworks like Junit, Mockito and \nPowermock. \n\uf06c Working knowledge of IBM WebSphere, Oracle WebLogic, JBoss and Apache Tomcat. \n\uf06c Excellent hands-on experience on XML, Parsing, SAX and DOM API \nDeepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | \nSecurity (5X) \nProfessional Certifications: \nDeepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | \nSecurity (5X) \nAmazon Web Services (AWS) \nS. No. \nCertification \nLicense Id \nValid From \nValid To \n1.  AWS Certified Solutions Architect - \nProfessional \nXQGQ1HVC2FF41ZGW \nJuly 2017 \nJuly 2020 \n2.  AWS Certified DevOps Engineer \u2013 \nProfessional \nHCR6VZGCKB44QZSN \nJune 2017 \nJune 2020 \n3.  AWS Certified Big Data Specialist \nCS2PR9X1CBV1Q2C2 \nAugust 2017 \nAugust 2020 \n4.  AWS Certified Advanced \nNetworking - Specialist \n9BGBWCV1JBQ111WQ \nApril 2018 \nApril 2021 \n5.  AWS Certified Security Specialist \n6QTRYP32B2E1193J \nMay 2019 \nMay 2022 \n6.  AWS Certified Solutions Architect - \nAssociate \nHQQG41112M1EQY9Q \nMay 2017 \nMay 2020 \n7.  AWS Certified SysOps Administrator \n- Associate \nRHDWRB4C2E111PK6 \nMay 2017 \nMay 2020 \n8.  AWS Certified Developer - \nAssociate \nR5SGQFSCKJ1E1FKQ \nMay 2017 \nMay 2020 \nDevOps \n1.  Certified Kubernetes Application \nDeveloper \nCKAD-1900-0603-0100 \nApril 2019 \nApril 2021 \n2.  Certified Kubernetes Administrator \nCKA-1900-001918-0100 \nMay 2019 \nMay 2021 \n3.  Jenkins Certified Engineer 2018 \n13083895 \nMay 2019 \nNever expire \nData Engineering \n1.  Confluent Certified Developer For \nApache Kafka \n13263590 \nJune 2019 \nJune 2021 \nEnterprise Architecture \n1.  TOGAF 9.1 Certified Enterprise \nArchitect \n113895 \nDecember 2016 \nNever expire \nJava \n1.  Oracle Certified Professional, Java \nSE 8 Programmer \n248717390OCPJSE8 \nDecember 2016 \nNever expire \n2.  Oracle Certified Professional, Java \nSE 6 Programmer \n248717390OCPJSE6P \nNovember 2016 \nNever expire \n3.  Oracle Certified Associate, Java SE 8 \nProgrammer \nNovember 2016 \nNever expire \n4.  Oracle Certified Expert, Java EE 6 \nWeb Services Developer \n248717390OCEJEE6WSD \nDecember 2016 \nNever expire \n5.  Oracle SOA Suite 12c Certified \nImplementation Specialist \n248717390SOA12COPN \nNovember 2016 \nNever expire \n6.  Oracle Application Development \nFramework 11g Certified \nImplementation Specialist \n248717390OADF11GOPN \nNovember 2016 \nNever expire \nSecurity and others \n1.  Certified Information Systems \nSecurity Professional \n593702 \nFebruary 2017 \nFebruary 2020 \n2.  Certified Information Security \nManager \n1737256 \nJuly 2017 \nJanuary 2021 \n3.  PRINCE2\u00ae Foundation Certificate in \nProject Management \n5890489.20618308 \nDecember 2016 \nNever expire \n4.  ITIL\u00ae Foundation Certificate in IT \nService Management \n5890489.20617142 \nDecember 2016 \nNever expire \nDeepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | \nSecurity (5X) \n5.  Oracle Identity Governance Suite \n11g PS3 Certified Implementation \nSpecialist \n248717390IGS11GPS3OPN \nNovember 2016 \nNever expire \n6.  Oracle Access Management Suite \nPlus 11g Implementation Specialist \n248717390OAM11GOCS \nNovember 2016 \nNever expire \nStrong Functional Competency:- \nPresales, Proposal Writing, RFPs, Requirement Gathering, Analysis, Solution/Enterprise Architecture, Consulting, Design, \nDevelopment, Implementation, Leadership, Production roll-outs, Go-Live Procedures, Business Development, Client \nRelationship, Effort Estimation, Project Planning \nDomains \nBanking, Financial services and Insurance (BFSI), Telecom, Healthcare, Public Sector, Retail, Manufacturing \nEducation \n1. Masters of Science (MS) in Software Engineering from BITS PILANI (2007 Passed) \n2. Bachelor of Science (BSc) in Computers from Osmania University (2003 Passed) \nEmployement History \nS. No. \nEmployer Name \nLocation \nDesignation \nFrom  \nTo \n1.  Agilitics Pte Ltd \nVietnam \nTraining Architect \nMarch \n2019 \nTill date \n2.  Equinix Singapore (Contract) \nSingapore \nSenior Consultant \nOct 2017 \nMarch \n2019 \n3.  Hitachi Consulting Singapore (Contract) \nSingapore \nSr. Solutions Architect \nNov 2016 \nOct 2017 \n4.  ST Electronics Singapore (Contract) \nSingapore \nSr. Solutions Architect \nJul 2016 \nOct 2016 \n5.  Accenture Singapore (Contract) \nSingapore \nSolutions Architect \nNov 2015 \nJun 2016 \n6.  United Overseas Bank Limited (UOB) \nSingapore (Contract) \nSingapore \nSolutions Architect \nMar 2015 \nOct 2015 \n7.  Serene Corporation Australia Pty Ltd \nMelbourne, \nAustralia \nSolutions Architect \nApr 2013 \nDec 2014 \n8.  Everett India Pvt Ltd \nBengaluru, \nIndia \nSr. Consultant \nOct 2011 \nNov 2012 \n9.  PricewaterhouseCoopers Pvt Ltd \nBengaluru, \nIndia \nTech Lead \nMar 2010 \nAug 2011 \n10.  EMC2 India Pvt Ltd \nBengaluru, \nIndia \nSr. Application Developer \nJun 2009 \nMar 2010 \n11.  Oracle India Pvt Ltd \nBengaluru, \nIndia \nStaff Consultant \nMar 2008 \nJun 2009 \n12.  Wipro Technologies Pvt Ltd \nBengaluru, \nIndia \nSr. Software Engineer \nSep 2003 \nMar 2008 \nProjects \nDeepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | \nSecurity (5X) \nProject # 01: Equinix Singapore \n \u2013 Big Data, Cloud Architecture, UECP Portal Design and Development \nEmployer: Equinix Singapore (Contract) \nBackground: Development of complete Big Data and DevOps platform for a variety of their systems for data processing \nand analytics. \nTasks \n1. \nIngesting batch and streaming data from a variety of sources like weblogs, databases, real time sensors using \ntechnologies like Apache Sqoop, Flume and Kafka. \n2. \nPersisting the processed data into HDFS, Cassandra and ElasticSearch. \n3. \nTransforming/Analyzing the data using Pig, Hive and Apache Spark. \n4. \nStoring back the summarized result for reporting into PostgreSQL and HBase. \n5. \nSetting up complete CI/CD pipeline using Jenkins, Maven, GIT, Docker and Kubernetes. \n6. \nSetting up of Docker environment and Images for automation and developers. \n7. \nSetting up of Kubernetes cluster for deployment workflow to production. \n8. \nArchitect, Design and Deployment on to the AWS Cloud. \nRole: Big Data and DevOps Solutions Architect  \nDuration: Oct 2017 to March 2019 \nProject # 02: Ministry of Education (MOE) Singapore \nEmployer: Hitachi Consulting Singapore (Contract) \nBackground: Development of complete web applications/portal on AWS Cloud and performing continuous integration \nand delivery using various CI/CD tools. Setting up the whole AWS Infrastructure on various environments, TDD and \ndeployment. Advising and Consultation on Big Data projects for a few clients. \nTasks \n1. \nRequirement Gathering, Analysis, Architecting and Designing of the AWS + Big Data technologies on the cloud \n2. \nInstallation and configuration of application servers, development environments and various product in multiple \nenvironment in standalone as well as clustered mode on AWS Cloud \n3. \nSetting up of various AWS Elastic MaReduce components like Hadoop, Spark, Presto, Hue for Data Analytics. \n4. \nAlso, required setting up of DynamoDB, SQS, Kinesis RedShift for data processing and storage depending on the \nrequirement. \n5. \nConfiguring S3 and EC2 Services, setting up an Oracle RDS Services, setting up Elastic Load Balancer, Setting up \nof Route53 DNS Services. \n6. \nAdvising, Consulting and providing Architecture guidelines multiple Big Data Projects involving a few end clients. \n7. \nDeveloping Scala Programs within the Spark Ecosystem for data analysis and processing. \n8. \nEnabling and configuring CloudWatch and CloudTrails logs and alarms. \n9. \nSetting up CI/CD using AWS CodeCommit, CodeBuild and CodeDeploy. \n10. Setting up of Ansible to manage, build and replicate environments. Developing Ansible scripts to setup multiple \nenvironments. \n11. Architect, Design and Deployment on to the AWS Cloud. \nRole: Big Data and DevOps Solutions Architect \nDuration: November 2016 to Oct 2017 \nDeepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | \nSecurity (5X) \nProject # 03: Singapore Prison Services (SPS) \u2013 Big Data and Data Science and Machine Learning Project Implementation \n\u2013 Singapore Govt Project \nEmployer: ST Electronics Singapore (Contract) \nBackground: Setting up of Hadoop Hortonworks stack in 3 environments (PROD, DR and DEV). \nTasks \n1. \nInstallation and configuration of Hadoop HDFS ecosystem \u2013 HDFS, MapReduce and YARN \n2. \nInstalling and Configuring of Apache Spark Cluster in various environments. \n3. \nInstalling and Configuring of Apache HBase. \n4. \nInstalling and Configuring Apache Kafka Cluster. \n5. \nSetting up CI/CD pipeline using Maven, GIT, Jenkins for automated development and deployments. \nRole: Big Data and DevOps Solutions Architect \nDuration: July 2016 to October 2016 \nProject # 04: Ministry of Health Holdings (MOHH) (Singapore)  \nEmployer: Accenture Singapore (Contract) \nBackground: Migrating key applications to the AWS Cloud Infrastructure and also performing key analytics using Hadoop \necosystem like Apache Spark, Apache Hue, Developing Spark programs using Java and storing the result back to NoSQL \ndatabase. \nMajor Tasks \n1. \nArchitect, Design and Development of Big Data Infrastructure on AWS Cloud. \n2. \nSetting up all of Hortonworks Hadoop ecosystem for the client and creating program for data ingestion and \nanalysis. \n3. \nDeveloping programs using Scala for Data Analysis and processing. \n4. \nDeveloping a few components using Spark Streaming for real time data analysis and processing. \n5. \nStoring the processed data back to Cassandra and MySQL. \n6. \nSetting up CI/CD pipeline using Maven, GIT, Jenkins, Docker and Kubernetes for end to end deployment of code \nfrom development to production. \nRole: Big Data and DevOps Solutions Architect \nDuration: December 2015 to June 2016 \nProject # 05: United Overseas Bank (Singapore) \nEmployer: UOB Singapore (Contract) \nBackground: Migrating key applications to the AWS Cloud Infrastructure and performing key analytics using Hadoop \nElastic MapReduce like Apache Spark, Apache Hue, Developing Spark programs using Scala and storing the result back to \nNoSQL database. \nTasks \n1. \nArchitect, Design and Develop Complete Cloud Infrastructure on AWS Cloud using AWS Elastic MapReduce \ntechnologies. \n2. \nDeveloping and Running using Spark (Scala) on multiple Spark Clusters. \n3. \nSetting up of Kinesis Streams for analysis of Clickstream data. \n4. \nSetting up of Redshift for data analytics to be used by BAs. \n5. \nSetting up of Hive, Presto for data analysis and quick querying for data. \n6. \nSetting up of QuickSight for data visualization, graph and dashboard. \n7. \nSetting of Data Pipeline for some of the data movement requirements between AWS components. \nRole: Big Data and DevOps Solutions Architect \nDuration: April 2015 to October 2015 \nDeepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | \nSecurity (5X) \nProject # 06: ANZ Bank (Australia) \nEmployer: Serene Corporation (Contract) \nBackground: Big Data Analysis on-premises setup.   \nTasks \n1. \nInstalling a Hadoop HDFS on clusters of systems dedicated for data analysis and storage. \n2. \nDeveloping Spark Scala programs for data analysis. \n3. \nIntegrating MySQL with Hadoop. \n4. \nDeveloping HiveQL Queries. \n5. \nInstalling and integrating HBase and Cassandra with Hadoop for Data storage. \n6. \nSetting up of Apache Drill for querying  \n7. \nSetting up Zookeeper, Oozie and Apache Ambari. \n8. \nSetting up of Basic AWS Services like Ec2, S3, ELB, VPC, Nat Instances, Route 53, Auto Scaling Groups and Launch \nConfiguration etc. \n9. \nAlso, involved in frontend and backend technologies for their portal development and providing APIs for the \nmobile app GoMoney's integration. \nRole: Big Data and DevOps Solutions Architect \nDuration: April 2013 to December 2014 \nPrevious Clients of Note (DevOps, Java, Java EE, CI/CD, TDD) :- \nEverything Everywhere (UK), TARGET, EMC2, Cisco, Washington Mutual, SuperValu \nProject # 06: TARGET (US based Retail Client) \nEmployer: PriceWaterhouseCoopers  \nTasks \n1. \nBuilding the Web Services server side and client side components for integration with User Management and \nRole Management Products. \n2. \nDevelopment of JUnit based Tests for java code and SoapUI based web services test code for web services layer. \n3. \nUsage of static and dynamic code coverage tools like FindBugs, CheckStyle, JProfiler etc. \nRole: Team Lead  \nDuration: Mar 2010 to Aug 2011 \nProject # 07: Cisco (US based Telecom Client) \nEmployer: Oracle Corporation  \nTasks \n1. \nDevelopment of JMS based solutions wherein XML Messages were published to JMS Queues. \n2. \nDevelopment of Message Driven Beans to consume and parse these messages using XML DOM based API. \n3. \nRegister the messages in staging table. \n4. \nPut Sequencing logic with XML messages to consume these messages in order. \n5. \nCRUD operations using JDBC API. \nRole: Senior Developer  \nDuration: April 2008 to June 2009 \nProject # 08: SuperValu (US based Retail Client) \nDeepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | \nSecurity (5X) \nEmployer: Wipro Technologies \nTasks \n1. \nEnd-to-End E-Commerce Development for SuperValu Client (B2A,B2B and B2C) using Java/J2EE technologies like \nServlets,JSP,JDBC,JNDI, JMS,HTML,CSS and Javascript. \n2. \nJunit Testing of all the Java Components. \n3. \nSetting up the build environments using Apache ANT build. \n4. \nSetting up the Web Projects (war) onto WebLogic Servers \n5. \nTuning the various components like databases, application servers, web servers, JVM etc. for maximum \nperformance. \nRole: Senior Developer  \nDuration: April 2005 to Mar 2008 ", "educations": [{"id": 5688, "gpa": null, "major": "K\u1ef9 thu\u1eadt ph\u1ea7n m\u1ec1m", "school": "BITS PILANI", "diploma": "Masters", "end_time": "2007-01-01T00:00:00Z", "conf_score": 0.7630396636158997, "start_time": null, "picklist_major": "IT - Hardware/Networking", "major_categories": "M\u00e1y t\u00ednh v\u00e0 c\u00f4ng ngh\u1ec7 th\u00f4ng tin", "major_categories_detail": "M\u00e1y t\u00ednh"}, {"id": 5689, "gpa": null, "major": "Computers", "school": "Osmania University", "diploma": "Bachelors", "end_time": "2003-01-01T00:00:00Z", "conf_score": 0.7630396636158997, "start_time": null, "picklist_major": "Other", "major_categories": "", "major_categories_detail": ""}], "experiences": [{"id": 19013, "detail": "", "company": "Agilitics Pte Ltd", "end_time": "2019-12-31T00:00:00Z", "industry": "Other", "position": "Training Architect", "conf_score": 0.7599593402313279, "start_time": "2019-03-01T00:00:00Z"}, {"id": 19014, "detail": "", "company": "Equinix Singapore (Contract)", "end_time": "2019-03-31T00:00:00Z", "industry": "Other", "position": "Senior Consultant", "conf_score": 0.7687020555050932, "start_time": "2017-10-01T00:00:00Z"}, {"id": 19015, "detail": "", "company": "Hitachi Consulting Singapore (Contract)", "end_time": "2017-10-31T00:00:00Z", "industry": "Other", "position": "Sr. Solutions Architect", "conf_score": 0.7687020555050932, "start_time": "2016-11-01T00:00:00Z"}, {"id": 19016, "detail": "", "company": "ST Electronics Singapore (Contract)", "end_time": "2016-10-31T00:00:00Z", "industry": "Other", "position": "Sr. Solutions Architect", "conf_score": 0.7687020555050932, "start_time": "2016-07-01T00:00:00Z"}, {"id": 19017, "detail": "(Contract) Background: Migrating key applications to the AWS Cloud Infrastructure and also performing key analytics using Hadoop ecosystem like Apache Spark, Apache Hue, Developing Spark programs using Java and storing the result back to NoSQL database.\nMajor Tasks 1. Architect, Design and Development of Big Data Infrastructure on AWS Cloud. 2. Setting up all of Hortonworks Hadoop ecosystem for the client and creating program for data ingestion and analysis.\n3. Developing programs using Scala for Data Analysis and processing.\n4. Developing a few components using Spark Streaming for real time data analysis and processing.\n5. Storing the processed data back to Cassandra and MySQL. 6. Setting up CI/CD pipeline using Maven, GIT, Jenkins, Docker and Kubernetes for end to end deployment of code from development to production.\nRole:", "company": "Accenture Singapore", "end_time": "2016-06-30T00:00:00Z", "industry": "Other", "position": "Big Data and DevOps Solutions Architect", "conf_score": 0.8673904863521351, "start_time": "2015-12-01T00:00:00Z"}, {"id": 19018, "detail": "", "company": "Accenture Singapore (Contract)", "end_time": "2016-06-30T00:00:00Z", "industry": "Other", "position": "Solutions Architect", "conf_score": 0.7687020555050932, "start_time": "2015-11-01T00:00:00Z"}, {"id": 19019, "detail": "(Contract) Background: Migrating key applications to the AWS Cloud Infrastructure and performing key analytics using Hadoop Elastic MapReduce like Apache Spark, Apache Hue, Developing Spark programs using Scala and storing the result back to\nNoSQL database.\nTasks 1. Architect, Design and Develop Complete Cloud Infrastructure on AWS Cloud using AWS Elastic MapReduce technologies.\n2. Developing and Running using Spark (Scala) on multiple Spark Clusters. 3. Setting up of Kinesis Streams for analysis of Clickstream data.\n4. Setting up of Redshift for data analytics to be used by BAs. 5. Setting up of Hive, Presto for data analysis and quick querying for data.\n6. Setting up of QuickSight for data visualization, graph and dashboard.\n7. Setting of Data Pipeline for some of the data movement requirements between AWS components.\nRole: Deepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | Security (5X)", "company": "UOB Singapore", "end_time": "2015-10-31T00:00:00Z", "industry": "Other", "position": "Big Data and DevOps Solutions Architect", "conf_score": 0.8577725998630228, "start_time": "2015-04-01T00:00:00Z"}, {"id": 19020, "detail": "", "company": "United Overseas Bank Limited (UOB)", "end_time": "2015-10-31T00:00:00Z", "industry": "Banking", "position": "Solutions Architect", "conf_score": 0.7933391211367344, "start_time": "2015-03-01T00:00:00Z"}, {"id": 19021, "detail": "", "company": "7. Serene Corporation Australia Pty Ltd", "end_time": "2014-12-31T00:00:00Z", "industry": "Other", "position": "Solutions Architect", "conf_score": 0.9178288400194387, "start_time": "2013-04-01T00:00:00Z"}, {"id": 19022, "detail": "", "company": "8. Everett India Pvt Ltd", "end_time": "2012-11-30T00:00:00Z", "industry": "Other", "position": "Sr. Consultant", "conf_score": 0.8785399304683831, "start_time": "2011-10-01T00:00:00Z"}, {"id": 19023, "detail": "", "company": "PricewaterhouseCoopers Pvt Ltd", "end_time": "2011-08-31T00:00:00Z", "industry": "Other", "position": "Tech Lead", "conf_score": 0.9085716158340945, "start_time": "2010-03-01T00:00:00Z"}, {"id": 19024, "detail": "", "company": "EMC2 India Pvt Ltd", "end_time": "2010-03-31T00:00:00Z", "industry": "Other", "position": "Sr. Application Developer", "conf_score": 0.8967422729948165, "start_time": "2009-06-01T00:00:00Z"}, {"id": 19025, "detail": "Tasks 1. Development of JMS based solutions wherein XML Messages were published to JMS Queues. 2. Development of Message Driven Beans to consume and parse these messages using XML DOM based API. 3. Register the messages in staging table.\n4. Put Sequencing logic with XML messages to consume these messages in order.\n5. CRUD operations using JDBC API. Role: Deepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS | Security (5X)", "company": "Oracle Corporation", "end_time": "2009-06-30T00:00:00Z", "industry": "Other", "position": "Senior Developer", "conf_score": 0.9178288400194387, "start_time": "2008-04-01T00:00:00Z"}, {"id": 19026, "detail": "", "company": "Oracle India Pvt Ltd", "end_time": "2009-06-30T00:00:00Z", "industry": "Other", "position": "Staff Consultant", "conf_score": 0.9178288400194387, "start_time": "2008-03-01T00:00:00Z"}, {"id": 19027, "detail": "Tasks 1. End-to-End E-Commerce Development for SuperValu Client (B2A,B2B and B2C) using Java/J2EE technologies like\nServlets,JSP,JDBC,JNDI, JMS,HTML,CSS and Javascript. 2. Junit Testing of all the Java Components. 3. Setting up the build environments using Apache ANT build.\n4. Setting up the Web Projects (war) onto WebLogic Servers 5. Tuning the various components like databases, application servers, web servers, JVM etc. for maximum performance.\nRole: Senior", "company": "Wipro Technologies", "end_time": "2008-03-31T00:00:00Z", "industry": "High Technology", "position": "Senior Developer", "conf_score": 0.9512443798773631, "start_time": "2005-04-01T00:00:00Z"}, {"id": 19028, "detail": "Deepak Dubey | | Big Data (3X) | AWS Cloud (8X) | DevOps (4X) | Java (6X) | Spring | Hibernate | Full Stack | ReactJS |", "company": "Wipro Technologies Pvt Ltd", "end_time": "2008-03-31T00:00:00Z", "industry": "High Technology", "position": "Sr. Software Engineer", "conf_score": 0.9259929444868232, "start_time": "2003-09-01T00:00:00Z"}], "address": null}